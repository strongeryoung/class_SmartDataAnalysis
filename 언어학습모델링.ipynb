{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strongeryoung/class_SmartDataAnalysis/blob/main/%EC%96%B8%EC%96%B4%ED%95%99%EC%8A%B5%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F5fSmK269pS"
      },
      "source": [
        "# Word Vector 란?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7CDaSKTaNDi"
      },
      "source": [
        "## Word Vector의 주요 내용\n",
        "\n",
        ">워드 벡터(Word Vector)는 단어를 연속 벡터 공간에 수치적으로 나타내는 방법입니다. 이는 단어 간의 의미적 관계를 포착하여 알고리즘이 언어를 더 효과적으로 이해하고 처리할 수 있게 합니다.\n",
        ">\n",
        ">기존의 단어 표현 방식 중 하나인 원핫 인코딩과 같은 전통적인 방법은 각 단어를 대부분 0과 단일한 1로 이루어진 희소하고 고차원 벡터로 표현합니다. 그에 비해 워드 벡터는 조밀하며 실수값을 포함하므로 단어의 의미를 보다 미묘하게 표현할 수 있습니다.\n",
        ">\n",
        ">워드 벡터는 주로 Word2Vec, GloVe(Global Vectors for Word Representation), FastText와 같은 비지도 학습 기술을 통해 학습됩니다. 이러한 모델은 대량의 텍스트 데이터에서 학습되며 비슷한 의미를 가진 단어를 벡터 공간에서 가까운 지점에 매핑하도록 합니다. 이는 모델이 단어 간의 유사성과 유추와 같은 의미적 관계를 파악할 수 있게 해줍니다.\n",
        "\n",
        "- CBOW 란?\n",
        "\n",
        ">CBOW (Continuous Bag of Words)는 Word2Vec이라는 워드 임베딩 알고리즘 중 하나의 모델입니다. Word2Vec은 단어를 벡터로 효과적으로 표현하기 위한 알고리즘으로, CBOW와 Skip-Gram이라는 두 가지 주요 아키텍처를 포함하고 있습니다.\n",
        ">\n",
        ">CBOW 모델은 특정 문맥 내에서 주어진 단어의 주변 단어들을 이용하여 해당 단어를 예측하는 방식으로 동작합니다. 즉, 주어진 문장이나 문맥 안에서 어떤 단어 주위에 어떤 단어들이 존재하는지를 고려하여 해당 단어를 예측하는 것이 목표입니다. CBOW는 문맥에서 주변 단어들의 평균 벡터를 사용하여 대상 단어를 예측합니다.\n",
        ">\n",
        ">CBOW는 주로 작은 규모의 데이터셋에서 효과적이며, 단어의 의미를 캡처하는 데 유용합니다. Word2Vec과 같은 워드 임베딩 모델들은 비지도 학습 방식으로 대량의 텍스트 데이터에서 학습되며, 이를 통해 단어 간의 의미적 관계를 잘 파악할 수 있게 됩니다.\n",
        "\n",
        "- Skip-Gram\n",
        "\n",
        ">Skip-Gram 모델은 CBOW와는 반대로 특정 단어를 가지고 주변 단어를 예측하는 방식으로 동작합니다. 즉, 특정 단어를 입력으로 받아서 그 단어 주변에 나타날 수 있는 주변 단어들을 예측하려고 합니다. Skip-Gram은 특정 단어와 그 주변 단어 사이의 관계를 학습하여 단어 간의 의미적 유사성을 포착하려고 합니다.\n",
        ">\n",
        ">Skip-Gram은 주로 대규모 데이터셋에서 효과적이며, 특히 풍부한 의미 정보를 가진 단어나 구절에 대한 표현을 잘 학습합니다. Skip-Gram과 CBOW는 각각 장단점이 있으며, 어떤 모델이 더 적합한지는 주어진 작업과 데이터에 따라 다를 수 있습니다.\n",
        "\n",
        "\n",
        "- Word Vector Example\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1guBz7L4uZwgviXTr3MwCPTcHUeHy66OH' height = 500 width = 600>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpp2NIvIzrvE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ul1rJ2Bzrys"
      },
      "source": [
        "#### 긍정부정 문장이 마침표로 분리되지 않은 문장을 마침표로 나누고 추가적인 긍부정 문장을 삽입하도록 한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMwLTmN_ei2k",
        "outputId": "b0075029-5c85-4c63-812b-ad1651033daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbx2-hgoablV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_tl84MOaunl",
        "outputId": "8af39c9b-f6c6-4a79-edbf-e072bf98d2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install konlpy #한국어 형태소분석 라이브러리\n",
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Twitter\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOQh1eNT_vvu"
      },
      "source": [
        "### 코드해설\n",
        "\n",
        "이 코드는 한글 텍스트 데이터를 사용하여 Word2Vec 모델을 만들기 위해 필요한 라이브러리를 설치하고 import하는 부분입니다. 각 라이브러리와 모듈에 대한 간단한 설명은 다음과 같습니다:\n",
        "\n",
        "1. **konlpy**: 한국어 자연어 처리를 위한 라이브러리로, 형태소 분석기 등 다양한 자연어 처리 도구를 제공합니다. `pip install konlpy` 명령을 통해 설치할 수 있습니다.\n",
        "\n",
        "2. **gensim.models.Word2Vec**: Word2Vec 모델을 구현한 Gensim 라이브러리의 모듈입니다. Word2Vec은 단어 간의 의미적 유사성을 학습하는 데 사용되는 워드 임베딩 기술 중 하나입니다.\n",
        "\n",
        "3. **konlpy.tag.Okt, konlpy.tag.Kkma, konlpy.tag.Twitter**: Konlpy에서 제공하는 형태소 분석기 중 몇 가지입니다. 예를 들어, Okt는 Open Korean Text로, Twitter는 한국어 트위터 데이터를 기반으로 만들어진 형태소 분석기입니다.\n",
        "\n",
        "4. **collections.Counter**: 파이썬의 내장 모듈로, 컨테이너에 저장된 데이터의 개수를 셀 때 사용됩니다. 주어진 데이터에서 각 단어의 빈도를 계산하는 데 사용될 수 있습니다.\n",
        "\n",
        "위의 코드는 필요한 라이브러리 및 모듈을 설치하고 import하는 부분이며, 이후에는 해당 라이브러리들을 사용하여 한글 텍스트 데이터를 전처리하고 Word2Vec 모델을 학습시킬 것으로 예상됩니다. Word2Vec 모델은 단어 간의 의미적 유사성을 학습하여 단어를 고차원 벡터로 표현하는데 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoNen_87eEiJ"
      },
      "outputs": [],
      "source": [
        "def check_len(str_1, len_num):\n",
        "  if len(str_1) > len_num:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def make_noun_doc(full_text):\n",
        "   # 문장 .를 기준으로 나눈다.\n",
        "    result_list = []\n",
        "    text_list = full_text.split('.')\n",
        "\n",
        "    for text in text_list:\n",
        "        twitter = Okt()\n",
        "        text_nouns = twitter.nouns(text)\n",
        "        #text_adj = twitter.adjective(text)\n",
        "        result_list.append(text_nouns)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "\n",
        "def make_all_doc(full_text):\n",
        "    okt=Okt()\n",
        "    #full_text.split('.')\n",
        "    result = okt.pos(full_text)\n",
        "    target_list = []\n",
        "    for w,p in result:\n",
        "         if p == \"Verb\" or p == \"Noun\" or p == 'Advjective' or p == 'Conjunction' or p == 'Josa': # 형태소 분석 결과 중에서 명사, 동사, 형용사를 분리하여 추출하도록 한다\n",
        "                target_list.append(w)\n",
        "    all_count = Counter(target_list)\n",
        "    str_result = ' '.join(target_list)\n",
        "    return  [target_list]\n",
        "\n",
        "def get_senti_score(text, dictionary):\n",
        "    text_nouns, str_result, noun_count = make_all_doc(text)\n",
        "    print(text_nouns)\n",
        "    sent_df = dictionary\n",
        "    try:\n",
        "        tmp_df = sent_df.loc[text_nouns] # text_nouns는 index리스트!\n",
        "        tmp_df= tmp_df.dropna()\n",
        "        print(tmp_df)\n",
        "        result =  tmp_df[['pos', 'neg','neu_score','pos_score','neg_score']].sum().values.tolist()\n",
        "        print(result)\n",
        "    except:\n",
        "        result = [0.0,0.0,0.0]\n",
        "    return result\n",
        "\n",
        "def read_data(path, num):\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for i in range(num):\n",
        "        df = pd.read_excel(path.format(i))\n",
        "        temp_df = df.drop_duplicates(0)\n",
        "        df_list.append(temp_df)\n",
        "    return df_list\n",
        "\n",
        "def words_mean(sentence, vec_size, model):\n",
        "# sentence의 형태는 명사로 이루어진 한 문장 (str), vec_size는 내가 학습시킨 word2vec에서의 vec사이즈\n",
        "#   words = sentence.split()                                                    ## 입력된 문장을 space 기준으로 분리시킨다, 여기서는 [단어,단어,단어] 형태로 전환되었기 때문에 ''기준으로 분리할 필요가 없다\n",
        "    vec_sum = np.zeros(vec_size, dtype=float)                                   ## 사전에 없는 경우를 대비하여 0 행렬을 형성한다\n",
        "  # word2vec voca set\n",
        "    word_vectors = model.wv                                                     ##  W2V이 학습한 단어의  set 이다. (일종의 사전역활을 한다)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:                                                ## 학습된 사전안에서 존재하는지 여부를 판단한다.\n",
        "            count += 1\n",
        "            vec_sum = np.add(vec_sum, model.wv[word])                           ## 1차원 행렬이다 !!, 사전에서 단어의 존재여부를 확인하여 존재 할경우 벡터값을 입력해준다. 없으면 넘어간다\n",
        "\n",
        "    if count > 0:                                                               ## count를 측정하여 평균을 계산한다.\n",
        "        vec_mean = vec_sum/count\n",
        "    else:\n",
        "        vec_mean = vec_sum                                                      ## 만약 count가 0이면 vec_mean = vec_sum 이다\n",
        "\n",
        "    return vec_mean.tolist()\n",
        "\n",
        "# [[],[],[]]\n",
        "def concat_list(news):                                                          ## [[문장:단어,단어,단어], [문장:단어,단어,단어], [문장:단어,단어,단어]] -->기사 형태의 리스트 구조를 합해서 [단어,단어,단어,단어,단어,단어] --> 기사 형태로 전환시킨다\n",
        "    result_list = []\n",
        "    for sen in news:\n",
        "        result_list = result_list + sen\n",
        "    return result_list\n",
        "\n",
        "def remove_words(words_list, stop_words):\n",
        "    new_words_list = []\n",
        "    word_list = words_list[0]\n",
        "    for word in word_list:\n",
        "        if word not in stop_words:\n",
        "            new_words_list.append(word)\n",
        "    return [new_words_list]\n",
        "\n",
        "\n",
        "def get_sentence(text_data):\n",
        "    text_list = []\n",
        "    for i in range(0,len(text_data)):\n",
        "        temp_list = text_data.iloc[i]['text'].split('.')\n",
        "        for txt in temp_list:\n",
        "            if txt != '':\n",
        "                text_list.append((txt, text_data.iloc[i]['score']))\n",
        "    temp_df = pd.DataFrame(text_list,columns = ['text', 'score'])\n",
        "    return temp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXqFk1AzAeA4"
      },
      "source": [
        "### 주요 함수 코드 해설\n",
        "\n",
        "**def make_noun_doc(full_text)**\n",
        "\n",
        "이 코드는 한글 텍스트에서 명사를 추출하는 함수인 `make_noun_doc`를 정의하는 파이썬 코드입니다. 아래는 코드의 각 부분에 대한 설명입니다:\n",
        "\n",
        "```python\n",
        "def make_noun_doc(full_text):\n",
        "    # 문장 .를 기준으로 나눈다.\n",
        "    result_list = []\n",
        "    text_list = full_text.split('.')\n",
        "```\n",
        "\n",
        "- `make_noun_doc` 함수를 정의합니다. 이 함수는 입력으로 받은 `full_text`라는 문자열에서 명사를 추출하여 리스트로 반환하는 역할을 합니다.\n",
        "- `result_list`라는 빈 리스트를 생성합니다. 추출된 명사들을 담을 예정입니다.\n",
        "- `full_text`를 문장 단위로 분리하기 위해 `split('.')`을 사용하여 `text_list`에 저장합니다.\n",
        "\n",
        "```python\n",
        "    for text in text_list:\n",
        "        twitter = Okt()\n",
        "        text_nouns = twitter.nouns(text)\n",
        "        #text_adj = twitter.adjective(text)\n",
        "        result_list.append(text_nouns)\n",
        "```\n",
        "\n",
        "- `text_list`에 있는 각각의 문장에 대해 반복문을 실행합니다.\n",
        "- `Okt()`를 사용하여 Twitter 형태소 분석기 객체를 생성합니다.\n",
        "- `twitter.nouns(text)`를 사용하여 현재 문장에서 명사만 추출하여 `text_nouns`에 저장합니다.\n",
        "- 추출된 명사를 `result_list`에 추가합니다.\n",
        "\n",
        "```python\n",
        "    return result_list\n",
        "```\n",
        "\n",
        "- 최종적으로, 모든 문장에 대한 명사 추출이 완료된 `result_list`를 반환합니다.\n",
        "\n",
        "\n",
        "\n",
        "**def make_all_doc(full_text)**\n",
        "\n",
        "이 코드는 주어진 한글 텍스트에서 명사, 동사, 형용사 등 일부 품사에 해당하는 단어를 추출하고, 각 단어의 빈도를 계산하여 리스트로 반환하는 함수인 `make_all_doc`를 정의합니다. 아래는 코드의 각 부분에 대한 설명입니다:\n",
        "\n",
        "```python\n",
        "def make_all_doc(full_text):\n",
        "    okt = Okt()\n",
        "    result = okt.pos(full_text)\n",
        "    target_list = []\n",
        "    for w, p in result:\n",
        "        if p == \"Verb\" or p == \"Noun\" or p == 'Adjective' or p == 'Conjunction' or p == 'Josa':\n",
        "            # 형태소 분석 결과 중에서 명사, 동사, 형용사 등을 추출하여 target_list에 추가합니다.\n",
        "            target_list.append(w)\n",
        "\n",
        "    all_count = Counter(target_list)\n",
        "    # Counter를 사용하여 각 단어의 빈도를 계산합니다.\n",
        "    str_result = ' '.join(target_list)\n",
        "    # target_list의 단어들을 공백으로 구분하여 하나의 문자열로 변환합니다.\n",
        "    return [target_list]\n",
        "```\n",
        "\n",
        "- `Okt()`를 사용하여 Twitter 형태소 분석기 객체를 생성합니다.\n",
        "- `okt.pos(full_text)`를 사용하여 형태소 분석을 수행하고, 단어와 그에 해당하는 품사 정보를 `result`에 저장합니다.\n",
        "- 각 단어와 품사에 대해 반복문을 실행하면서, 명사(Noun), 동사(Verb), 형용사(Adjective), 접속사(Conjunction), 조사(Josa)에 해당하는 단어들을 `target_list`에 추가합니다.\n",
        "- `Counter(target_list)`를 사용하여 각 단어의 빈도를 계산한 후, 결과를 `all_count`에 저장합니다. (이 부분은 현재 코드에서 활용되지 않고 있습니다.)\n",
        "- `target_list`의 단어들을 공백으로 구분하여 하나의 문자열인 `str_result`로 변환합니다.\n",
        "- 최종적으로, 명사, 동사, 형용사 등을 포함한 단어들이 담긴 리스트를 반환합니다.\n",
        "\n",
        "\n",
        "\n",
        "**def words_mean(sentence, vec_size, model)**\n",
        "\n",
        "이 코드는 주어진 한 문장의 명사들에 대한 Word2Vec 벡터의 평균을 계산하는 함수인 `words_mean`을 정의합니다. 아래는 코드의 각 부분에 대한 설명입니다:\n",
        "\n",
        "```python\n",
        "def words_mean(sentence, vec_size, model):\n",
        "    vec_sum = np.zeros(vec_size, dtype=float)  # 0으로 초기화된 크기가 vec_size인 행렬을 생성합니다.\n",
        "\n",
        "    word_vectors = model.wv  # Word2Vec 모델에서 단어 벡터들을 가져옵니다.\n",
        "\n",
        "    count = 0  # 문장에서 Word2Vec 모델에 있는 단어의 수를 세기 위한 변수입니다.\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:  # 단어가 Word2Vec 모델의 단어 집합에 있는지 확인합니다.\n",
        "            count += 1\n",
        "            vec_sum = np.add(vec_sum, model.wv[word])  # 단어의 벡터를 더합니다.\n",
        "\n",
        "    if count > 0:\n",
        "        vec_mean = vec_sum / count  # 평균 벡터를 계산합니다.\n",
        "    else:\n",
        "        vec_mean = vec_sum  # 만약 count가 0이면 평균 대신 더한 벡터 그대로를 반환합니다.\n",
        "\n",
        "    return vec_mean.tolist()  # 결과를 리스트 형태로 반환합니다.\n",
        "```\n",
        "\n",
        "- `vec_sum`: 결과 벡터의 초기값으로 사용할 0으로 초기화된 행렬을 생성합니다.\n",
        "- `word_vectors`: Word2Vec 모델에서 단어 벡터들을 가져오기 위해 `model.wv`를 사용합니다.\n",
        "- `count`: Word2Vec 모델에 있는 단어의 수를 세기 위한 변수로, 반복문을 통해 확인합니다.\n",
        "- 반복문에서 각 단어에 대해, 해당 단어가 Word2Vec 모델에 있는 경우 (존재하는 단어인 경우) 그 단어의 벡터를 `vec_sum`에 더합니다.\n",
        "- 만약 `count`가 0보다 크면, `vec_sum`을 `count`로 나누어 평균 벡터 `vec_mean`을 계산합니다.\n",
        "- 그렇지 않으면, `vec_mean`은 `vec_sum` 그대로이며, 이는 해당 문장에 Word2Vec 모델에 있는 단어가 하나도 없는 경우입니다.\n",
        "- 최종적으로, 결과 벡터를 리스트로 변환하여 반환합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TREjva5G9mk"
      },
      "outputs": [],
      "source": [
        "## 학습문장 import\n",
        "path = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/개인폴더/이종훈/관광관련긍부정문장_V2.xlsx'\n",
        "pos_text_df = pd.read_excel(path,sheet_name = 'positive', header = None)\n",
        "neg_text_df = pd.read_excel(path,sheet_name = 'negative', header = None)\n",
        "\n",
        "pos_text_df['score'] = 1\n",
        "neg_text_df['score'] = 0\n",
        "\n",
        "text_df = pd.concat([pos_text_df, neg_text_df])\n",
        "text_df.columns = ['text', 'score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVx1CThjh1FU"
      },
      "outputs": [],
      "source": [
        "## text 정재 3단계 과정을 거친다\n",
        "text_df['words'] = text_df['text'].apply(lambda x:re.sub('[^A-Za-z가-힣]', ' ', str(x)))\n",
        "text_df['words'] = text_df['words'].apply(lambda x : re.sub('\\n', ' ', x))\n",
        "text_df['words'] = text_df['words'].apply(lambda x : ' '.join(x.split()))\n",
        "text_df['elements'] = text_df['words'].apply(lambda x : make_all_doc(x))        ## 명사, 동사, 형용사를 분리하는 작업을 수행하도록 한다\n",
        "\n",
        "stop_words = ['광고', '신문']\n",
        "text_df['elements_clean'] = text_df['elements'].apply(lambda x : remove_words(x, stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-IkOxOPi-md"
      },
      "outputs": [],
      "source": [
        "news_df = text_df\n",
        "corpus = news_df['elements_clean'].sum()\n",
        "\n",
        "vec_size = 300                                                                  ## word 벡터 사이즈는 입력\n",
        "window_num = 3\n",
        "model = Word2Vec(corpus, vector_size = vec_size, window = window_num, min_count=1, workers=4)\n",
        "\n",
        "text_df['news_nouns'] = text_df['elements_clean'].apply(lambda x : concat_list(x))\n",
        "text_df['vec_mean'] = text_df['news_nouns'].apply(lambda x : words_mean(x, vec_size, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol39iTzMrldc"
      },
      "source": [
        "주어진 코드는 Gensim 라이브러리를 사용하여 Word2Vec 모델을 만드는 파이썬 코드입니다. 아래는 코드의 각 부분을 해석한 것입니다:\n",
        "\n",
        "```python\n",
        "vec_size = 300  # 단어 벡터의 크기를 300으로 설정합니다.\n",
        "window_num = 3  # 윈도우 크기를 3으로 설정합니다.\n",
        "model = Word2Vec(corpus, vector_size=vec_size, window=window_num, min_count=1, workers=4)\n",
        "```\n",
        "\n",
        "- `vec_size = 300`: 단어 벡터의 크기를 나타냅니다. 각 단어는 300차원의 벡터로 표현됩니다.\n",
        "\n",
        "- `window_num = 3`: 윈도우 크기를 나타냅니다. 이는 모델이 학습할 때 고려하는 현재 단어 주변의 문맥 범위를 결정합니다. 여기서는 현재 단어 앞 뒤로 3개의 단어까지 고려합니다.\n",
        "\n",
        "- `model = Word2Vec(corpus, vector_size=vec_size, window=window_num, min_count=1, workers=4)`: Word2Vec 모델을 생성하는 부분입니다. 이때 사용되는 매개변수는 다음과 같습니다.\n",
        "\n",
        "  - `corpus`: 단어 벡터를 학습할 문장 또는 토큰의 목록입니다.\n",
        "  \n",
        "  - `vector_size`: 단어 벡터의 크기를 나타냅니다. 위에서 설정한 `vec_size` 값인 300이 사용됩니다.\n",
        "\n",
        "  - `window`: 모델이 학습할 때 고려하는 현재 단어 주변의 문맥 범위를 나타냅니다. 위에서 설정한 `window_num` 값인 3이 사용됩니다.\n",
        "\n",
        "  - `min_count`: 이 값을 설정하면 주어진 횟수보다 적게 나오는 단어는 무시됩니다. 여기서는 1로 설정되어 있어 모든 단어를 고려합니다.\n",
        "\n",
        "  - `workers`: 모델을 학습할 때 사용할 CPU 코어의 수를 나타냅니다. 위에서는 4개의 코어를 사용하도록 설정되어 있습니다.\n",
        "\n",
        "이 코드를 실행하면 `model` 객체를 사용하여 특정 단어의 벡터 표현을 얻거나 유사한 단어를 찾는 등의 작업을 수행할 수 있습니다. 추가 질문이나 궁금한 점이 있으면 언제든 물어보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "-fnEjTEKpP6E",
        "outputId": "5023dca6-a974-47f1-ce16-19c5e1ebc174"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-061c5966-03f9-404c-9147-f2a7d010a5a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "      <th>words</th>\n",
              "      <th>elements</th>\n",
              "      <th>elements_clean</th>\n",
              "      <th>news_nouns</th>\n",
              "      <th>vec_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1. 이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다</td>\n",
              "      <td>[[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]</td>\n",
              "      <td>[[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]</td>\n",
              "      <td>[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]</td>\n",
              "      <td>[-0.023401618602552583, 0.1512138911389879, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2. 역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다</td>\n",
              "      <td>[[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]</td>\n",
              "      <td>[[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]</td>\n",
              "      <td>[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]</td>\n",
              "      <td>[-0.018930749417367308, 0.12985834089869802, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3. 독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다</td>\n",
              "      <td>[[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]</td>\n",
              "      <td>[[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]</td>\n",
              "      <td>[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]</td>\n",
              "      <td>[-0.019887753153770147, 0.13103156731548635, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4. 관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다</td>\n",
              "      <td>[[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]</td>\n",
              "      <td>[[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]</td>\n",
              "      <td>[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]</td>\n",
              "      <td>[-0.0221238439771696, 0.14974648845236516, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5. 다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요.</td>\n",
              "      <td>1</td>\n",
              "      <td>다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요</td>\n",
              "      <td>[[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...</td>\n",
              "      <td>[[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...</td>\n",
              "      <td>[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할,...</td>\n",
              "      <td>[-0.019431392309100678, 0.12394320930664739, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-061c5966-03f9-404c-9147-f2a7d010a5a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-061c5966-03f9-404c-9147-f2a7d010a5a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-061c5966-03f9-404c-9147-f2a7d010a5a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0551dfa8-689a-4b87-bac1-dfb24ffdadd6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0551dfa8-689a-4b87-bac1-dfb24ffdadd6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0551dfa8-689a-4b87-bac1-dfb24ffdadd6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             text  score  \\\n",
              "0          1. 이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다.      1   \n",
              "1      2. 역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다.      1   \n",
              "2     3. 독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다.      1   \n",
              "3     4. 관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다.      1   \n",
              "4  5. 다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요.      1   \n",
              "\n",
              "                                        words  \\\n",
              "0          이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다   \n",
              "1      역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다   \n",
              "2     독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다   \n",
              "3     관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다   \n",
              "4  다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요   \n",
              "\n",
              "                                            elements  \\\n",
              "0                     [[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]   \n",
              "1       [[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]   \n",
              "2          [[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]   \n",
              "3  [[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]   \n",
              "4  [[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...   \n",
              "\n",
              "                                      elements_clean  \\\n",
              "0                     [[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]   \n",
              "1       [[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]   \n",
              "2          [[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]   \n",
              "3  [[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]   \n",
              "4  [[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...   \n",
              "\n",
              "                                          news_nouns  \\\n",
              "0                       [곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]   \n",
              "1         [역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]   \n",
              "2            [문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]   \n",
              "3    [관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]   \n",
              "4  [채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할,...   \n",
              "\n",
              "                                            vec_mean  \n",
              "0  [-0.023401618602552583, 0.1512138911389879, -0...  \n",
              "1  [-0.018930749417367308, 0.12985834089869802, -...  \n",
              "2  [-0.019887753153770147, 0.13103156731548635, -...  \n",
              "3  [-0.0221238439771696, 0.14974648845236516, -0....  \n",
              "4  [-0.019431392309100678, 0.12394320930664739, -...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPLh8SHiZ7Ai"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(text_df['vec_mean'].values.tolist())\n",
        "y = text_df['score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_VadIwWzf-K"
      },
      "source": [
        "# Machine Learning에 대한 간략한 소개\n",
        "\n",
        "머신러닝은 컴퓨터가 데이터에서 학습하고 패턴을 식별하여 일련의 작업을 자동으로 수행하는 인공 지능(AI)의 한 분야입니다. 기본적으로, 머신러닝은 명시적인 프로그래밍 없이도 컴퓨터가 학습하고 개선할 수 있도록 하는 기술을 제공합니다.\n",
        "\n",
        "머신러닝은 크게 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 나뉩니다.\n",
        "\n",
        "1. **지도 학습 (Supervised Learning):** 모델을 학습시키기 위해 입력과 해당 출력을 사용합니다. 이는 모델이 주어[링크 텍스트](https://)진 입력 데이터에 대한 정확한 출력을 예측하도록 학습하는 방법입니다. 분류(Classification)와 회귀(Regression)가 이에 해당합니다.\n",
        "\n",
        "2. **비지도 학습 (Unsupervised Learning):** 학습 데이터에 명시적인 출력이 제공되지 않습니다. 대신, 모델은 데이터의 구조나 패턴을 발견하려고 합니다. 군집화(Clustering)나 차원 축소(Dimensionality Reduction)가 여기에 속합니다.\n",
        "\n",
        "3. **강화 학습 (Reinforcement Learning):** 에이전트라고 불리는 학습 주체가 환경과 상호작용하며, 어떤 행동을 취하고 그 결과에 따른 보상 또는 패널티를 받아 학습합니다. 목표는 누적된 보상을 최대화하는 것입니다.\n",
        "\n",
        "머신러닝은 다양한 응용 분야에서 사용되며, 예를 들면 음성 인식, 이미지 인식, 자연어 처리, 게임에서의 전략 개발, 의료 진단, 금융 예측 등이 있습니다. 알고리즘은 데이터로부터 학습하고 예측 또는 의사 결정을 수행함으로써 점차적으로 성능을 향상시킵니다.\n",
        "\n",
        "- Word Vector Example\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1tYPv5K3_kmG9gBRZJR3_pYNrYX7VOBLQ' height = 500 width = 800>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABCaFFUWzgFv"
      },
      "source": [
        "# 주요 Machine Learning Model 간략 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9sgpH0P7omH"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "로지스틱 회귀(Logistic Regression)는 주로 이진 분류(Binary Classification) 문제를 다루는 머신러닝 알고리즘 중 하나입니다. 이는 이름이 회귀(Regression)라고 붙어있지만, 실제로는 분류(Classification) 알고리즘에 속합니다.\n",
        "\n",
        "### 동작 원리:\n",
        "\n",
        "1. **로지스틱 함수 (Sigmoid 함수):** 로지스틱 회귀는 로지스틱 함수, 또는 시그모이드 함수라 불리는 특별한 함수를 사용합니다. 시그모이드 함수는 모든 실수 입력값을 0과 1 사이의 값으로 압축시키는 역할을 합니다.\n",
        "\n",
        "   $  \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "   여기서 \\(z\\)는 입력 변수의 가중치 합과 편향(bias)을 나타내며, 시그모이드 함수를 통과한 결과는 해당 입력이 클래스 1에 속할 확률을 나타냅니다.\n",
        "\n",
        "2. **학습과 손실 함수:** 로지스틱 회귀는 주어진 입력에 대한 예측과 실제 레이블 간의 차이를 최소화하는 방향으로 모델의 가중치와 편향을 학습합니다. 이를 위해 일반적으로 로그 손실(Log Loss)이라 불리는 손실 함수를 사용합니다.\n",
        "\n",
        "   $  \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $\n",
        "\n",
        "   여기서 \\(y_i\\)는 실제 레이블, \\(\\hat{y}_i\\)는 모델의 예측값입니다.\n",
        "\n",
        "### 특징:\n",
        "\n",
        "- **이진 분류:** 로지스틱 회귀는 기본적으로 두 개의 클래스를 분류하는 데 사용됩니다.\n",
        "  \n",
        "- **선형 결정 경계:** 로지스틱 회귀의 결정 경계는 선형입니다. 즉, 입력 공간을 두 영역으로 나누는 하나의 직선이나 초평면입니다.\n",
        "\n",
        "- **각 특성의 가중치:** 모델은 각 입력 특성에 대한 가중치를 학습하여 해당 특성이 출력에 미치는 영향을 나타냅니다.\n",
        "\n",
        "로지스틱 회귀는 간단하면서도 효과적인 알고리즘으로, 특히 선형 결정 경계가 적절한 경우에 유용하게 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSzS9oNzgNi"
      },
      "source": [
        "## XG Boost (참고 : https://yamalab.tistory.com/215)\n",
        "\n",
        "XGBoost(Extreme Gradient Boosting)는 그라디언트 부스팅(Gradient Boosting) 알고리즘의 하나로, 트리 기반의 앙상블 학습 알고리즘입니다. XGBoost는 대회와 실제 응용 프로그램에서 널리 사용되며, 다양한 유형의 데이터셋에서 높은 성능을 보입니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **부스팅 알고리즘:** 부스팅은 약한 학습자(Weak Learner)라 불리는 간단한 모델들을 순차적으로 학습시켜 강력한 모델을 만들어가는 앙상블 기법입니다. XGBoost는 이러한 부스팅 기법을 사용하여 여러 결정 트리를 조합하여 강력한 모델을 형성합니다.\n",
        "\n",
        "2. **트리 알고리즘:** XGBoost는 결정 트리를 기반으로 하는 앙상블 모델입니다. 각 트리는 이전 트리의 오차를 보완하도록 학습됩니다.\n",
        "\n",
        "3. **그라디언트 부스팅:** 그라디언트 부스팅은 손실 함수의 그라디언트(기울기)를 이용하여 모델을 학습하는 방식입니다. XGBoost는 그라디언트 부스팅의 효율적인 구현을 통해 높은 성능을 제공합니다.\n",
        "\n",
        "4. **규제(Regularization):** XGBoost는 규제를 통해 모델의 복잡성을 제어합니다. 이는 모델이 과적합(Overfitting)되는 것을 방지하고 일반적으로 좋은 성능을 내도록 도움을 줍니다.\n",
        "\n",
        "5. **특징 중요도:** XGBoost는 학습된 모델에서 각 특징이 예측에 얼마나 중요한 역할을 하는지를 계산하여 특징 중요도를 제공합니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "XGBoost는 다양한 분야에서 사용됩니다. 주로 다음과 같은 문제에 적용됩니다:\n",
        "\n",
        "- **분류(Classification):** 이진 분류나 다중 클래스 분류 문제에 사용됩니다.\n",
        "  \n",
        "- **회귀(Regression):** 수치형 목표 변수를 예측하는 회귀 문제에도 적용 가능합니다.\n",
        "  \n",
        "- **랭킹(Ranking):** 검색 엔진이나 추천 시스템에서 사용되어 개체의 상대적인 중요도를 학습할 수 있습니다.\n",
        "\n",
        "XGBoost는 성능이 우수하고 많은 하이퍼파라미터 튜닝 옵션을 제공하여 다양한 데이터셋에 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM28FVv_73V_"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "랜덤 포레스트(Random Forest)는 앙상블 학습(Ensemble Learning) 기법 중 하나로, 의사 결정 트리(Decision Tree)의 앙상블을 구성하여 강력하고 안정적인 모델을 만드는 알고리즘입니다. 여러 개의 의사 결정 트리를 만들고, 각 트리의 예측을 종합함으로써 과적합을 줄이고 높은 성능을 달성할 수 있습니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **부트스트랩 샘플링 (Bootstrap Sampling):** 랜덤 포레스트는 각 트리를 학습할 때, 데이터의 일부를 무작위로 선택하여 사용합니다. 이는 중복이 허용되는 샘플링으로, 부트스트랩 샘플링이라고 불립니다. 이를 통해 각 트리가 다양한 데이터 부분 집합에서 학습되어 다양성을 증가시킵니다.\n",
        "\n",
        "2. **랜덤 특성 선택 (Random Feature Selection):** 각 의사 결정 트리를 학습할 때, 특성들 중 일부를 무작위로 선택하여 사용합니다. 이는 트리 간의 상관 관계를 줄이고 다양성을 증가시킵니다.\n",
        "\n",
        "3. **앙상블 학습:** 여러 개의 의사 결정 트리를 독립적으로 학습하고, 각 트리의 예측을 평균이나 다수결 등의 방식으로 종합하여 최종 예측을 만듭니다.\n",
        "\n",
        "4. **고차원 데이터와 높은 차원의 특성을 다룰 수 있음:** 다양한 데이터 유형 및 특성에 적용 가능하며, 범주형과 수치형 특성 모두 다룰 수 있습니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "랜덤 포레스트는 다양한 분야에서 사용되며, 특히 다음과 같은 상황에서 효과적입니다:\n",
        "\n",
        "- **분류 및 회귀 문제:** 둘 다 다룰 수 있으며, 분류 문제에서 특히 높은 성능을 보입니다.\n",
        "\n",
        "- **이상치 탐지:** 이상치 탐지에도 적용 가능하며, 개별 트리의 예측을 기반으로 이상치를 식별할 수 있습니다.\n",
        "\n",
        "- **텍스트 및 이미지 데이터:** 다양한 데이터 유형에 적용 가능하며, 특히 텍스트와 이미지 데이터에서 효과적입니다.\n",
        "\n",
        "랜덤 포레스트는 강력하면서도 쉽게 사용할 수 있는 모델로, 많은 상황에서 좋은 기본 모델로 활용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lsITqFN7626"
      },
      "source": [
        "##  Ada Boost\n",
        "\n",
        "AdaBoost(Adaptive Boosting)는 약한 학습자(Weak Learner)들을 조합하여 강력한 학습 모델을 만드는 앙상블 학습 알고리즘 중 하나입니다. AdaBoost는 부분 데이터셋에 가중치를 부여하면서 순차적으로 모델을 학습시키고, 학습이 어려운 샘플에 더 많은 가중치를 주어 다음 모델이 이를 더 잘 학습할 수 있도록 하는 특징을 가지고 있습니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **가중치를 부여한 샘플링:** AdaBoost는 각 학습 단계에서 이전 모델이 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 이를 더욱 잘 학습할 수 있도록 합니다.\n",
        "\n",
        "2. **약한 학습자 사용:** AdaBoost는 약한 학습자(Weak Learner)를 사용합니다. 이는 학습 오차가 50% 미만인, 조금 더 나은 성능을 가진 모델로서 일반적으로 의사 결정 트리(Decision Tree)가 사용됩니다.\n",
        "\n",
        "3. **앙상블 학습:** 각 학습 단계에서 새로운 모델을 추가하고, 이전 모델들의 가중 평균을 통해 최종 예측을 만들어냅니다.\n",
        "\n",
        "4. **이진 분류에 적합:** 주로 이진 분류 문제에 사용되며, 다중 클래스 분류 문제에는 확장하여 사용할 수 있습니다.\n",
        "\n",
        "### 동작 원리:\n",
        "\n",
        "1. **첫 번째 모델 학습:** 초기에는 모든 샘플에 동일한 가중치를 부여하고, 첫 번째 모델을 학습시킵니다.\n",
        "\n",
        "2. **오분류된 샘플에 가중치 부여:** 이전 모델이 잘못 분류한 샘플에 대해 가중치를 높여줍니다.\n",
        "\n",
        "3. **새로운 모델 학습:** 새로운 가중치를 적용하여 다음 모델을 학습시킵니다.\n",
        "\n",
        "4. **반복:** 위 과정을 미리 정의한 횟수(또는 정확도가 충분히 높아질 때까지) 반복합니다.\n",
        "\n",
        "5. **앙상블 구성:** 각 모델의 예측을 가중 평균하여 최종 예측을 만듭니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "AdaBoost는 다양한 분야에서 사용되며, 특히 다음과 같은 상황에서 효과적입니다:\n",
        "\n",
        "- **이진 분류 문제:** 주로 이진 분류 문제에서 사용되며, 약한 학습자를 사용하므로 간단하고 빠른 모델을 구성할 수 있습니다.\n",
        "\n",
        "- **얼굴 검출, 객체 감지:** 이미지 처리에서 얼굴 검출이나 객체 감지와 같은 작업에 사용될 수 있습니다.\n",
        "\n",
        "- **자연어 처리(NLP):** 텍스트 분류 문제에서도 효과적으로 사용될 수 있습니다.\n",
        "\n",
        "AdaBoost는 간단하면서도 성능이 좋은 앙상블 학습 알고리즘 중 하나이지만, 이상치(outliers)에 민감할 수 있으며, 노이즈 데이터에 과적합할 가능성이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Foh4STzgWR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EjSTFySZ7-s",
        "outputId": "3e92a7af-a260-4c80-d174-d0194151143a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "accuracy with training 52.94%\n",
            "accuracy with testing 53.85%\n",
            "0.5286496775697304\n",
            "[[ 83 128]\n",
            " [ 82 162]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.39      0.44       211\n",
            "           1       0.56      0.66      0.61       244\n",
            "\n",
            "    accuracy                           0.54       455\n",
            "   macro avg       0.53      0.53      0.52       455\n",
            "weighted avg       0.53      0.54      0.53       455\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test =  train_test_split(X, y, random_state=0, test_size=0.2)\n",
        "\n",
        "log_clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter = 5000, multi_class='multinomial').fit(X_train, y_train)\n",
        "log_result = log_clf.predict(X_train)\n",
        "#print(log_clf.predict_proba(X_test))\n",
        "log_prob = log_clf.predict_proba(X_test)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(log_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(log_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,log_clf.predict(X_test)))\n",
        "\n",
        "log_prob = log_clf.predict_proba(X_test)\n",
        "\n",
        "y_pred = log_clf.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLfPIqbZ1B_H"
      },
      "source": [
        "이 코드는 로지스틱 회귀(Logistic Regression) 분류 모델을 사용하여 이진 또는 다중 클래스 분류 문제를 다루고 있습니다. 코드의 각 부분을 설명하겠습니다:\n",
        "\n",
        "1. **데이터 분할:**\n",
        "   - `train_test_split` 함수를 사용하여 전체 데이터를 학습 데이터 (`X_train`, `y_train`)와 테스트 데이터 (`X_test`, `y_test`)로 나눕니다.\n",
        "   - `random_state`는 데이터를 분할할 때 사용되는 난수 시드입니다. `test_size`는 테스트 세트의 비율을 나타냅니다.\n",
        "\n",
        "2. **로지스틱 회귀 모델 학습:**\n",
        "   - `LogisticRegression` 클래스를 초기화하고, `fit` 메서드를 사용하여 학습 데이터에 모델을 훈련시킵니다.\n",
        "   - `random_state`는 내부적으로 사용되는 난수 시드, `solver`는 최적화 알고리즘, `max_iter`는 최대 반복 횟수, `multi_class`는 다중 클래스 분류 방법을 지정합니다.\n",
        "\n",
        "3. **모델 평가 및 예측:**\n",
        "   - 학습된 모델을 사용하여 학습 데이터와 테스트 데이터에 대한 예측을 수행합니다.\n",
        "   - 정확도를 출력하고, ROC AUC 스코어를 계산하여 출력합니다.\n",
        "\n",
        "4. **모델 예측 및 평가:**\n",
        "   - `predict_proba` 메서드를 사용하여 테스트 데이터에 대한 예측 확률을 계산합니다.\n",
        "   - 모델의 예측값을 출력하고, `confusion_matrix`를 사용하여 혼동 행렬을 출력합니다.\n",
        "   - `classification_report`를 사용하여 정밀도, 재현율, F1-score 등을 포함한 분류 보고서를 출력합니다.\n",
        "\n",
        "이 코드는 로지스틱 회귀 모델을 사용하여 분류 작업을 수행하고, 모델의 정확도 및 성능을 평가하는 과정을 담고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0t5aXsZzHX4"
      },
      "source": [
        "# 머신러닝 분류모델 성능 평가\n",
        "\n",
        "`confusion_matrix` 함수는 분류 모델의 성능을 평가하기 위한 도구 중 하나로, 실제값과 모델의 예측값을 기반으로 4가지 항목으로 이루어진 행렬을 생성합니다. 주로 이진 분류(Binary Classification)에서 사용되며, 다중 클래스 분류(Multiclass Classification)에서도 확장하여 사용할 수 있습니다.\n",
        "\n",
        "이 행렬은 다음과 같은 4개의 항목으로 구성됩니다:\n",
        "\n",
        "| Actual Positive    | Actual Negative |    |\n",
        "|--------------------|-----------------|----|\n",
        "| Predicted Positive | TP              | FP |\n",
        "| Predicted Negative | FN              | TN |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. **True Positive (TP):** 실제 값이 Positive이고, 모델이 Positive로 정확하게 예측한 경우입니다.\n",
        "2. **False Positive (FP):** 실제 값이 Negative인데 모델이 Positive로 잘못 예측한 경우입니다. (예측이 틀린 경우)\n",
        "3. **True Negative (TN):** 실제 값이 Negative이고, 모델이 Negative로 정확하게 예측한 경우입니다.\n",
        "4. **False Negative (FN):** 실제 값이 Positive인데 모델이 Negative로 잘못 예측한 경우입니다. (예측이 틀린 경우)\n",
        "\n",
        "이러한 정보를 사용하여 모델의 정확성, 정밀도, 재현율, F1 스코어 등을 계산할 수 있습니다.\n",
        "\n",
        "\n",
        "`Precision`, `Recall` 및 `F1-score`는 분류 모델의 성능을 측정하기 위한 여러 지표 중 일부입니다.\n",
        "\n",
        "1. **Precision (정밀도):**\n",
        "   - 정밀도는 모델이 Positive로 예측한 것 중에서 실제로 Positive인 비율을 나타냅니다.\n",
        "   - 공식: Precision = TP / (TP + FP)\n",
        "   - Precision이 높을수록 모델이 Positive로 예측한 것 중에서 실제로 Positive인 경우가 높아집니다.\n",
        "\n",
        "2. **Recall (재현율 또는 민감도):**\n",
        "   - 재현율은 실제로 Positive인 것 중에서 모델이 Positive로 예측한 비율을 나타냅니다.\n",
        "   - 공식: Recall = TP / (TP + FN)\n",
        "   - Recall이 높을수록 실제로 Positive인 것 중에서 모델이 Positive로 예측한 경우가 높아집니다.\n",
        "\n",
        "3. **F1-score:**\n",
        "   - F1-score는 정밀도와 재현율의 조화 평균입니다. 정밀도와 재현율 사이의 균형을 나타내는 지표로 사용됩니다.\n",
        "   - 공식: F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "   - F1-score는 두 지표의 조화 평균이므로, 두 값 중 하나가 낮을 경우에도 양쪽 모두 고려됩니다.\n",
        "\n",
        "이 세 가지 지표는 각각 다른 측면에서 모델의 성능을 평가합니다. 모델의 목적과 상황에 따라 어떤 지표가 더 중요한지 결정할 수 있습니다. Precision은 거짓 양성을 줄이는 데 중점을 두고, Recall은 거짓 음성을 줄이는 데 중점을 둡니다. F1-score는 두 가지를 균형있게 고려합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_uR3xaYaEAg"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "xgb_clf = XGBClassifier(max_depth=3, n_estimators=250)\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "#print(X.columns.tolist())\n",
        "#print(xgb_clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(xgb_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(xgb_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,xgb_clf.predict(X_test)))\n",
        "\n",
        "xgb_prob = xgb_clf.predict_proba(X_test)\n",
        "\n",
        "# print(xgb_clf.score(X_test, y_test))\n",
        "\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZVIfcqE0g-r"
      },
      "source": [
        "이 코드는 XGBoost(Xtreme Gradient Boosting) 분류기를 사용하여 분류 모델을 학습하고 그 성능을 평가하는 일련의 작업을 수행합니다. 코드의 각 부분을 설명하겠습니다:\n",
        "\n",
        "1. **라이브러리 임포트:**\n",
        "   - `XGBClassifier`: XGBoost 분류기를 제공하는 XGBoost 라이브러리에서 가져옵니다.\n",
        "   - `RandomForestClassifier`: RandomForest 분류기를 제공하는 scikit-learn 라이브러리에서 가져옵니다.\n",
        "   - `plot_importance`: XGBoost 모델의 피처 중요도를 시각화하기 위한 함수를 가져옵니다.\n",
        "   - `train_test_split`: 데이터를 학습 및 테스트 세트로 분할하기 위한 함수를 가져옵니다.\n",
        "   - `matplotlib.pyplot`: 그래프 및 시각화를 위한 라이브러리를 가져옵니다.\n",
        "   - `roc_auc_score`, `confusion_matrix`, `classification_report`: 성능 평가를 위한 함수들을 가져옵니다.\n",
        "\n",
        "2. **모델 학습:**\n",
        "   - `XGBClassifier`를 초기화하고, `max_depth`와 `n_estimators`를 설정한 후 `fit` 메서드를 사용하여 학습 데이터에 모델을 훈련시킵니다.\n",
        "\n",
        "3. **정확도 및 ROC AUC 스코어 출력:**\n",
        "   - 훈련 데이터와 테스트 데이터에 대한 정확도를 출력합니다.\n",
        "   - `roc_auc_score` 함수를 사용하여 ROC AUC 스코어를 계산하고 출력합니다.\n",
        "\n",
        "4. **모델 예측 및 평가:**\n",
        "   - `predict_proba` 메서드를 사용하여 테스트 데이터에 대한 예측 확률을 계산합니다.\n",
        "   - 모델의 예측값을 출력하고, `confusion_matrix`를 사용하여 혼동 행렬을 출력합니다.\n",
        "   - `classification_report`를 사용하여 정밀도, 재현율, F1-score 등을 포함한 분류 보고서를 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "218O2ISxaVpU"
      },
      "outputs": [],
      "source": [
        "RF_clf = RandomForestClassifier(n_estimators=250, random_state=0, max_depth=3)\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "RF_clf.fit(X_train, y_train)\n",
        "#print(X_train.columns.tolist())\n",
        "#print(RF_clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(RF_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(RF_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,RF_clf.predict(X_test)))\n",
        "\n",
        "y_pred = RF_clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF3wvPlQ1Pr5"
      },
      "source": [
        "이 코드는 RandomForestClassifier를 사용하여 랜덤 포레스트(Random Forest) 분류 모델을 학습하고 평가하는 작업을 수행합니다. 코드의 각 부분을 설명하겠습니다:\n",
        "\n",
        "1. **랜덤 포레스트 분류 모델 초기화:**\n",
        "   - `RandomForestClassifier`를 초기화하고, `n_estimators`는 생성할 트리의 개수, `random_state`는 난수 시드, `max_depth`는 각 트리의 최대 깊이를 나타냅니다.\n",
        "\n",
        "2. **모델 학습:**\n",
        "   - `fit` 메서드를 사용하여 학습 데이터에 모델을 훈련시킵니다.\n",
        "\n",
        "3. **정확도 출력:**\n",
        "   - 학습된 모델을 사용하여 학습 데이터와 테스트 데이터에 대한 정확도를 출력합니다.\n",
        "\n",
        "4. **ROC AUC 스코어 출력:**\n",
        "   - `roc_auc_score` 함수를 사용하여 ROC AUC 스코어를 계산하고 출력합니다.\n",
        "\n",
        "5. **모델 예측 및 평가:**\n",
        "   - 모델을 사용하여 테스트 데이터에 대한 예측을 수행하고, 혼동 행렬과 분류 보고서를 출력합니다.\n",
        "\n",
        "이 코드는 RandomForestClassifier를 사용하여 분류 작업을 수행하고, 모델의 정확도 및 성능을 평가하는 과정을 담고 있습니다. 랜덤 포레스트는 여러 결정 트리를 결합하여 안정적이고 강력한 모델을 형성하므로, 다양한 분류 문제에서 효과적으로 사용될 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bVIsyp6TaYKg",
        "outputId": "72000af8-9beb-4902-8725-ff389e9c26df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "==================================================\n",
            "accuracy with training 71.91%\n",
            "accuracy with testing 66.15%\n",
            "0.6587871960220653\n",
            "[[131  80]\n",
            " [ 74 170]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.62      0.63       211\n",
            "           1       0.68      0.70      0.69       244\n",
            "\n",
            "    accuracy                           0.66       455\n",
            "   macro avg       0.66      0.66      0.66       455\n",
            "weighted avg       0.66      0.66      0.66       455\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(n_estimators=200, learning_rate=0.1,random_state=0)\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "ada_clf.fit(X_train, y_train)\n",
        "#print(clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(ada_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(ada_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,ada_clf.predict(X_test)))\n",
        "ADA_prob = ada_clf.predict_proba(X)\n",
        "\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zctbRprm14KB"
      },
      "source": [
        "이 코드는 AdaBoostClassifier를 사용하여 AdaBoost(Adaptive Boosting) 분류 모델을 학습하고 평가하는 작업을 수행합니다. 코드의 각 부분을 설명하겠습니다:\n",
        "\n",
        "1. **AdaBoost 분류 모델 초기화:**\n",
        "   - `AdaBoostClassifier`를 초기화하고, `n_estimators`는 생성할 기본 분류기(weak learner)의 개수, `learning_rate`는 각 weak learner의 기여 정도를 나타내는 학습률입니다. `random_state`는 난수 시드입니다.\n",
        "\n",
        "2. **모델 학습:**\n",
        "   - `fit` 메서드를 사용하여 학습 데이터에 모델을 훈련시킵니다.\n",
        "\n",
        "3. **정확도 출력:**\n",
        "   - 학습된 모델을 사용하여 학습 데이터와 테스트 데이터에 대한 정확도를 출력합니다.\n",
        "\n",
        "4. **ROC AUC 스코어 출력:**\n",
        "   - `roc_auc_score` 함수를 사용하여 ROC AUC 스코어를 계산하고 출력합니다.\n",
        "\n",
        "5. **모델 예측 및 평가:**\n",
        "   - `predict_proba` 메서드를 사용하여 전체 데이터셋 `X`에 대한 예측 확률을 계산합니다.\n",
        "   - 모델을 사용하여 테스트 데이터에 대한 예측을 수행하고, 혼동 행렬과 분류 보고서를 출력합니다.\n",
        "\n",
        "AdaBoost는 약한 학습자들을 결합하여 강력한 모델을 형성하는 앙상블 학습 알고리즘 중 하나입니다. AdaBoost는 이전에 잘못 분류된 샘플에 가중치를 높게 두어, 다음 분류기가 이를 보완하도록 하는 특징이 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rvYxRU1Xacch",
        "outputId": "7c6e3e1a-4d50-4168-a9f2-eacb31a1243a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/Week_13/word_model/ada_model_300_3_2023-12-09.pkl']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_info = datetime.today().strftime(format=\"%Y-%m-%d\")\n",
        "\n",
        "## 학습한 W2V 모델 저장\n",
        "model.save('/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/Week_13/word_model/word2vec_{}_{}_{}.model'.format(vec_size, window_num, date_info))\n",
        "\n",
        "## 학습한 머신러닝\n",
        "from joblib import dump, load\n",
        "\n",
        "fname2 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/Week_13/word_model/xgb_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(xgb_clf, fname2)\n",
        "\n",
        "fname3 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/Week_13/word_model/RF_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(RF_clf, fname3)\n",
        "\n",
        "fname4 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302/Week_13/word_model/ada_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(ada_clf, fname4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BKpxEMzNspUP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}