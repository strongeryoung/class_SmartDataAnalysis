{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strongeryoung/class_SmartDataAnalysis/blob/main/%EC%96%B8%EC%96%B4%ED%95%99%EC%8A%B5%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Vector 란?"
      ],
      "metadata": {
        "id": "2F5fSmK269pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Vector의 주요 내용\n",
        "\n",
        ">워드 벡터(Word Vector)는 단어를 연속 벡터 공간에 수치적으로 나타내는 방법입니다. 이는 단어 간의 의미적 관계를 포착하여 알고리즘이 언어를 더 효과적으로 이해하고 처리할 수 있게 합니다.\n",
        ">\n",
        ">기존의 단어 표현 방식 중 하나인 원핫 인코딩과 같은 전통적인 방법은 각 단어를 대부분 0과 단일한 1로 이루어진 희소하고 고차원 벡터로 표현합니다. 그에 비해 워드 벡터는 조밀하며 실수값을 포함하므로 단어의 의미를 보다 미묘하게 표현할 수 있습니다.\n",
        ">\n",
        ">워드 벡터는 주로 Word2Vec, GloVe(Global Vectors for Word Representation), FastText와 같은 비지도 학습 기술을 통해 학습됩니다. 이러한 모델은 대량의 텍스트 데이터에서 학습되며 비슷한 의미를 가진 단어를 벡터 공간에서 가까운 지점에 매핑하도록 합니다. 이는 모델이 단어 간의 유사성과 유추와 같은 의미적 관계를 파악할 수 있게 해줍니다.\n",
        "\n",
        "- CBOW 란?\n",
        "\n",
        ">CBOW (Continuous Bag of Words)는 Word2Vec이라는 워드 임베딩 알고리즘 중 하나의 모델입니다. Word2Vec은 단어를 벡터로 효과적으로 표현하기 위한 알고리즘으로, CBOW와 Skip-Gram이라는 두 가지 주요 아키텍처를 포함하고 있습니다.\n",
        ">\n",
        ">CBOW 모델은 특정 문맥 내에서 주어진 단어의 주변 단어들을 이용하여 해당 단어를 예측하는 방식으로 동작합니다. 즉, 주어진 문장이나 문맥 안에서 어떤 단어 주위에 어떤 단어들이 존재하는지를 고려하여 해당 단어를 예측하는 것이 목표입니다. CBOW는 문맥에서 주변 단어들의 평균 벡터를 사용하여 대상 단어를 예측합니다.\n",
        ">\n",
        ">CBOW는 주로 작은 규모의 데이터셋에서 효과적이며, 단어의 의미를 캡처하는 데 유용합니다. Word2Vec과 같은 워드 임베딩 모델들은 비지도 학습 방식으로 대량의 텍스트 데이터에서 학습되며, 이를 통해 단어 간의 의미적 관계를 잘 파악할 수 있게 됩니다.\n",
        "\n",
        "- Skip-Gram\n",
        "\n",
        ">Skip-Gram 모델은 CBOW와는 반대로 특정 단어를 가지고 주변 단어를 예측하는 방식으로 동작합니다. 즉, 특정 단어를 입력으로 받아서 그 단어 주변에 나타날 수 있는 주변 단어들을 예측하려고 합니다. Skip-Gram은 특정 단어와 그 주변 단어 사이의 관계를 학습하여 단어 간의 의미적 유사성을 포착하려고 합니다.\n",
        ">\n",
        ">Skip-Gram은 주로 대규모 데이터셋에서 효과적이며, 특히 풍부한 의미 정보를 가진 단어나 구절에 대한 표현을 잘 학습합니다. Skip-Gram과 CBOW는 각각 장단점이 있으며, 어떤 모델이 더 적합한지는 주어진 작업과 데이터에 따라 다를 수 있습니다.\n",
        "\n",
        "\n",
        "- Word Vector Example\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1guBz7L4uZwgviXTr3MwCPTcHUeHy66OH' height = 500 width = 600>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "R7CDaSKTaNDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cpp2NIvIzrvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 긍정부정 문장이 마침표로 분리되지 않은 문장을 마침표로 나누고 추가적인 긍부정 문장을 삽입하도록 한다"
      ],
      "metadata": {
        "id": "9ul1rJ2Bzrys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMwLTmN_ei2k",
        "outputId": "0989e3b6-092a-4b12-8476-9b90415cdce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dbx2-hgoablV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install konlpy\n",
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Twitter\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import re\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_tl84MOaunl",
        "outputId": "8b8ce48e-b3b4-43d9-e82c-b0a41d3f9044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words(new_text):\n",
        "  stop_words = ['스탁론', '탁론', '개월', '무단', '저작권', '전날', '대한', '대해', '대출']\n",
        "  for stopword in stop_words:\n",
        "    if stopword in new_text:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def check_len(str_1, len_num):\n",
        "  if len(str_1) > len_num:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def make_noun_doc(full_text):\n",
        "   # 문장 .를 기준으로 나눈다.\n",
        "    result_list = []\n",
        "    text_list = full_text.split('.')\n",
        "\n",
        "    for text in text_list:\n",
        "        twitter = Okt()\n",
        "        text_nouns = twitter.nouns(text)\n",
        "        #text_adj = twitter.adjective(text)\n",
        "        result_list.append(text_nouns)\n",
        "\n",
        "    return result_list\n",
        "\n",
        "\n",
        "def make_all_doc(full_text):\n",
        "    okt=Okt()\n",
        "    #full_text.split('.')\n",
        "    result = okt.pos(full_text)\n",
        "    target_list = []\n",
        "    for w,p in result:\n",
        "         if p == \"Verb\" or p == \"Noun\" or p == 'Advjective' or p == 'Conjunction' or p == 'Josa': # 형태소 분석 결과 중에서 명사, 동사, 형용사를 분리하여 추출하도록 한다\n",
        "                target_list.append(w)\n",
        "    all_count = Counter(target_list)\n",
        "    str_result = ' '.join(target_list)\n",
        "    return  [target_list]\n",
        "\n",
        "def sentiment_analyst(df, len_num):\n",
        "\n",
        "    df[0] = df['text']\n",
        "    df[0] = df[0].drop_duplicates()\n",
        "    df = df.dropna()\n",
        "    df[0] = df[0].apply(lambda x: re.sub('[^가-힣0-9a-zA-Z\\\\s]', '', x))\n",
        "    df[0] = df[0].apply(lambda x: ' '.join(x.split()))\n",
        "\n",
        "    df['remove_word'] = df[0].apply(lambda x: remove_words(x))\n",
        "    df = df[df['remove_word'] == True]\n",
        "\n",
        "    #print(df)\n",
        "    df['len_check'] = df[0].apply(lambda x: check_len(x, len_num))\n",
        "    df = df[df['len_check'] == True]\n",
        "    txt_list = df[0].values.tolist()\n",
        "\n",
        "     # str, int , float, list, dict\n",
        "    noun_list = []\n",
        "    sentence = []\n",
        "    noun_cnt = []\n",
        "\n",
        "  #print(df)\n",
        "\n",
        "    for i in range(len(txt_list)):\n",
        "\n",
        "        text = txt_list[i]\n",
        "        nouns, str_, noun_count = make_noun_doc(text)\n",
        "        noun_list.append(nouns)\n",
        "        sentence.append(str_)\n",
        "        noun_cnt.append(noun_count)\n",
        "\n",
        "    noun_count_df = (pd.DataFrame(noun_cnt)).T.fillna(0)\n",
        "    noun_count_df['freq'] = noun_count_df.sum(axis=1)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_senti_score(text, dictionary):\n",
        "    text_nouns, str_result, noun_count = make_all_doc(text)\n",
        "    print(text_nouns)\n",
        "    sent_df = dictionary\n",
        "    try:\n",
        "        tmp_df = sent_df.loc[text_nouns] # text_nouns는 index리스트!\n",
        "        tmp_df= tmp_df.dropna()\n",
        "        print(tmp_df)\n",
        "        result =  tmp_df[['pos', 'neg','neu_score','pos_score','neg_score']].sum().values.tolist()\n",
        "        print(result)\n",
        "    except:\n",
        "        result = [0.0,0.0,0.0]\n",
        "    return result\n",
        "\n",
        "def read_data(path, num):\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for i in range(num):\n",
        "        df = pd.read_excel(path.format(i))\n",
        "        temp_df = df.drop_duplicates(0)\n",
        "        df_list.append(temp_df)\n",
        "    return df_list\n",
        "\n",
        "def words_mean(sentence, vec_size, model):\n",
        "# sentence의 형태는 명사로 이루어진 한 문장 (str), vec_size는 내가 학습시킨 word2vec에서의 vec사이즈\n",
        "#   words = sentence.split()                                                    ## 입력된 문장을 space 기준으로 분리시킨다, 여기서는 [단어,단어,단어] 형태로 전환되었기 때문에 ''기준으로 분리할 필요가 없다\n",
        "    vec_sum = np.zeros(vec_size, dtype=float)                                   ## 사전에 없는 경우를 대비하여 0 행렬을 형성한다\n",
        "  # word2vec voca set\n",
        "    word_vectors = model.wv                                                     ##  W2V이 학습한 단어의  set 이다. (일종의 사전역활을 한다)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in word_vectors:                                                ## 학습된 사전안에서 존재하는지 여부를 판단한다.\n",
        "            count += 1\n",
        "            vec_sum = np.add(vec_sum, model.wv[word])                           ## 1차원 행렬이다 !!, 사전에서 단어의 존재여부를 확인하여 존재 할경우 벡터값을 입력해준다. 없으면 넘어간다\n",
        "\n",
        "    if count > 0:                                                               ## count를 측정하여 평균을 계산한다.\n",
        "        vec_mean = vec_sum/count\n",
        "    else:\n",
        "        vec_mean = vec_sum                                                      ## 만약 count가 0이면 vec_mean = vec_sum 이다\n",
        "\n",
        "    return vec_mean.tolist()\n",
        "\n",
        "# [[],[],[]]\n",
        "def concat_list(news):                                                          ## [[문장:단어,단어,단어], [문장:단어,단어,단어], [문장:단어,단어,단어]] -->기사 형태의 리스트 구조를 합해서 [단어,단어,단어,단어,단어,단어] --> 기사 형태로 전환시킨다\n",
        "    result_list = []\n",
        "    for sen in news:\n",
        "        result_list = result_list + sen\n",
        "    return result_list\n",
        "\n",
        "def remove_words(words_list, stop_words):\n",
        "    new_words_list = []\n",
        "    word_list = words_list[0]\n",
        "    for word in word_list:\n",
        "        if word not in stop_words:\n",
        "            new_words_list.append(word)\n",
        "    return [new_words_list]\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence(text_data):\n",
        "    text_list = []\n",
        "    for i in range(0,len(text_data)):\n",
        "        temp_list = text_data.iloc[i]['text'].split('.')\n",
        "        for txt in temp_list:\n",
        "            if txt != '':\n",
        "                text_list.append((txt, text_data.iloc[i]['score']))\n",
        "    temp_df = pd.DataFrame(text_list,columns = ['text', 'score'])\n",
        "    return temp_df"
      ],
      "metadata": {
        "id": "aoNen_87eEiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 학습문장 import\n",
        "path = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/관광관련긍부정문장_V2.xlsx'\n",
        "pos_text_df = pd.read_excel(path,sheet_name = 'positive', header = None)\n",
        "neg_text_df = pd.read_excel(path,sheet_name = 'negative', header = None)\n",
        "\n",
        "pos_text_df['score'] = 1\n",
        "neg_text_df['score'] = 0\n",
        "\n",
        "text_df = pd.concat([pos_text_df, neg_text_df])\n",
        "text_df.columns = ['text', 'score']"
      ],
      "metadata": {
        "id": "_TREjva5G9mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## text 정재 3단계 과정을 거친다\n",
        "text_df['words'] = text_df['text'].apply(lambda x:re.sub('[^A-Za-z가-힣]', ' ', str(x)))\n",
        "text_df['words'] = text_df['words'].apply(lambda x : re.sub('\\n', ' ', x))\n",
        "text_df['words'] = text_df['words'].apply(lambda x : ' '.join(x.split()))\n",
        "text_df['elements'] = text_df['words'].apply(lambda x : make_all_doc(x))        ## 명사, 동사, 형용사를 분리하는 작업을 수행하도록 한다\n",
        "\n",
        "stop_words = ['광고', '신문']\n",
        "text_df['elements_clean'] = text_df['elements'].apply(lambda x : remove_words(x, stop_words))"
      ],
      "metadata": {
        "id": "DVx1CThjh1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = text_df\n",
        "corpus = news_df['elements_clean'].sum()\n",
        "\n",
        "vec_size = 300                                                                  ## word 벡터 사이즈는 입력\n",
        "window_num = 3\n",
        "model = Word2Vec(corpus, vector_size = vec_size, window = window_num, min_count=1, workers=4)\n",
        "\n",
        "text_df['news_nouns'] = text_df['elements_clean'].apply(lambda x : concat_list(x))\n",
        "text_df['vec_mean'] = text_df['news_nouns'].apply(lambda x : words_mean(x, vec_size, model))"
      ],
      "metadata": {
        "id": "s-IkOxOPi-md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "-fnEjTEKpP6E",
        "outputId": "966b517b-a1db-4a63-b64d-35e7de7aa978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             text  score  \\\n",
              "0          1. 이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다.      1   \n",
              "1      2. 역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다.      1   \n",
              "2     3. 독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다.      1   \n",
              "3     4. 관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다.      1   \n",
              "4  5. 다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요.      1   \n",
              "\n",
              "                                        words  \\\n",
              "0          이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다   \n",
              "1      역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다   \n",
              "2     독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다   \n",
              "3     관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다   \n",
              "4  다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요   \n",
              "\n",
              "                                            elements  \\\n",
              "0                     [[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]   \n",
              "1       [[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]   \n",
              "2          [[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]   \n",
              "3  [[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]   \n",
              "4  [[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...   \n",
              "\n",
              "                                      elements_clean  \\\n",
              "0                     [[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]   \n",
              "1       [[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]   \n",
              "2          [[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]   \n",
              "3  [[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]   \n",
              "4  [[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...   \n",
              "\n",
              "                                          news_nouns  \\\n",
              "0                       [곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]   \n",
              "1         [역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]   \n",
              "2            [문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]   \n",
              "3    [관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]   \n",
              "4  [채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할,...   \n",
              "\n",
              "                                            vec_mean  \n",
              "0  [-0.007859457383996673, 0.09200211808950241, -...  \n",
              "1  [-0.00497701311674477, 0.07642861404879527, -0...  \n",
              "2  [-0.006338010415096174, 0.08018433198925447, -...  \n",
              "3  [-0.006672089373751078, 0.09085397734694804, -...  \n",
              "4  [-0.006725055328570306, 0.07545474045909942, -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4e9552e-f090-4230-b34c-a31c0138b149\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "      <th>words</th>\n",
              "      <th>elements</th>\n",
              "      <th>elements_clean</th>\n",
              "      <th>news_nouns</th>\n",
              "      <th>vec_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1. 이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>이곳은 아름다운 자연 경치와 함께 펼쳐지는 특별한 관광지입니다</td>\n",
              "      <td>[[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]</td>\n",
              "      <td>[[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]]</td>\n",
              "      <td>[곳, 은, 자연, 경치, 와, 펼쳐지는, 관광지]</td>\n",
              "      <td>[-0.007859457383996673, 0.09200211808950241, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2. 역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>역사적인 매력과 현대적인 편의시설이 조화롭게 어우러진 멋진 장소입니다</td>\n",
              "      <td>[[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]</td>\n",
              "      <td>[[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]]</td>\n",
              "      <td>[역사, 인, 매력, 과, 현대, 인, 편의, 시설, 이, 어우러진, 장소]</td>\n",
              "      <td>[-0.00497701311674477, 0.07642861404879527, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3. 독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>독특한 문화와 전통이 살아 숨쉬는 이곳은 정말로 놀라운 관광 명소입니다</td>\n",
              "      <td>[[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]</td>\n",
              "      <td>[[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]]</td>\n",
              "      <td>[문화, 와, 전통, 이, 살, 아, 숨쉬는, 곳, 은, 관광, 명소]</td>\n",
              "      <td>[-0.006338010415096174, 0.08018433198925447, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4. 관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>관광객들에게 감동을 안겨주는 이곳은 마치 동화 속 세계처럼 아름답습니다</td>\n",
              "      <td>[[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]</td>\n",
              "      <td>[[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]]</td>\n",
              "      <td>[관광객, 에게, 감동, 을, 안겨주는, 곳, 은, 마치, 동화, 속, 세계, 처럼]</td>\n",
              "      <td>[-0.006672089373751078, 0.09085397734694804, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5. 다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요.</td>\n",
              "      <td>1</td>\n",
              "      <td>다채로운 액티비티와 풍부한 볼거리로 가득한 이 관광지는 방문할 가치가 있어요</td>\n",
              "      <td>[[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...</td>\n",
              "      <td>[[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할...</td>\n",
              "      <td>[채, 로운, 액, 티비, 티, 와, 볼거리, 로, 이, 관광, 지는, 방문, 할,...</td>\n",
              "      <td>[-0.006725055328570306, 0.07545474045909942, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4e9552e-f090-4230-b34c-a31c0138b149')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4e9552e-f090-4230-b34c-a31c0138b149 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4e9552e-f090-4230-b34c-a31c0138b149');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bb4a8b58-48d6-48cc-90da-434c124bc47a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb4a8b58-48d6-48cc-90da-434c124bc47a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bb4a8b58-48d6-48cc-90da-434c124bc47a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(text_df['vec_mean'].values.tolist())\n",
        "y = text_df['score']"
      ],
      "metadata": {
        "id": "GPLh8SHiZ7Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning에 대한 간략한 소개\n",
        "\n",
        "머신러닝은 컴퓨터가 데이터에서 학습하고 패턴을 식별하여 일련의 작업을 자동으로 수행하는 인공 지능(AI)의 한 분야입니다. 기본적으로, 머신러닝은 명시적인 프로그래밍 없이도 컴퓨터가 학습하고 개선할 수 있도록 하는 기술을 제공합니다.\n",
        "\n",
        "머신러닝은 크게 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 나뉩니다.\n",
        "\n",
        "1. **지도 학습 (Supervised Learning):** 모델을 학습시키기 위해 입력과 해당 출력을 사용합니다. 이는 모델이 주어[링크 텍스트](https://)진 입력 데이터에 대한 정확한 출력을 예측하도록 학습하는 방법입니다. 분류(Classification)와 회귀(Regression)가 이에 해당합니다.\n",
        "\n",
        "2. **비지도 학습 (Unsupervised Learning):** 학습 데이터에 명시적인 출력이 제공되지 않습니다. 대신, 모델은 데이터의 구조나 패턴을 발견하려고 합니다. 군집화(Clustering)나 차원 축소(Dimensionality Reduction)가 여기에 속합니다.\n",
        "\n",
        "3. **강화 학습 (Reinforcement Learning):** 에이전트라고 불리는 학습 주체가 환경과 상호작용하며, 어떤 행동을 취하고 그 결과에 따른 보상 또는 패널티를 받아 학습합니다. 목표는 누적된 보상을 최대화하는 것입니다.\n",
        "\n",
        "머신러닝은 다양한 응용 분야에서 사용되며, 예를 들면 음성 인식, 이미지 인식, 자연어 처리, 게임에서의 전략 개발, 의료 진단, 금융 예측 등이 있습니다. 알고리즘은 데이터로부터 학습하고 예측 또는 의사 결정을 수행함으로써 점차적으로 성능을 향상시킵니다.\n",
        "\n",
        "- Word Vector Example\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1tYPv5K3_kmG9gBRZJR3_pYNrYX7VOBLQ' height = 500 width = 800>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "I_VadIwWzf-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 주요 Machine Learning Model 간략 소개"
      ],
      "metadata": {
        "id": "ABCaFFUWzgFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "로지스틱 회귀(Logistic Regression)는 주로 이진 분류(Binary Classification) 문제를 다루는 머신러닝 알고리즘 중 하나입니다. 이는 이름이 회귀(Regression)라고 붙어있지만, 실제로는 분류(Classification) 알고리즘에 속합니다.\n",
        "\n",
        "### 동작 원리:\n",
        "\n",
        "1. **로지스틱 함수 (Sigmoid 함수):** 로지스틱 회귀는 로지스틱 함수, 또는 시그모이드 함수라 불리는 특별한 함수를 사용합니다. 시그모이드 함수는 모든 실수 입력값을 0과 1 사이의 값으로 압축시키는 역할을 합니다.\n",
        "\n",
        "   $  \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "   여기서 \\(z\\)는 입력 변수의 가중치 합과 편향(bias)을 나타내며, 시그모이드 함수를 통과한 결과는 해당 입력이 클래스 1에 속할 확률을 나타냅니다.\n",
        "\n",
        "2. **학습과 손실 함수:** 로지스틱 회귀는 주어진 입력에 대한 예측과 실제 레이블 간의 차이를 최소화하는 방향으로 모델의 가중치와 편향을 학습합니다. 이를 위해 일반적으로 로그 손실(Log Loss)이라 불리는 손실 함수를 사용합니다.\n",
        "\n",
        "   $  \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $\n",
        "\n",
        "   여기서 \\(y_i\\)는 실제 레이블, \\(\\hat{y}_i\\)는 모델의 예측값입니다.\n",
        "\n",
        "### 특징:\n",
        "\n",
        "- **이진 분류:** 로지스틱 회귀는 기본적으로 두 개의 클래스를 분류하는 데 사용됩니다.\n",
        "  \n",
        "- **선형 결정 경계:** 로지스틱 회귀의 결정 경계는 선형입니다. 즉, 입력 공간을 두 영역으로 나누는 하나의 직선이나 초평면입니다.\n",
        "\n",
        "- **각 특성의 가중치:** 모델은 각 입력 특성에 대한 가중치를 학습하여 해당 특성이 출력에 미치는 영향을 나타냅니다.\n",
        "\n",
        "로지스틱 회귀는 간단하면서도 효과적인 알고리즘으로, 특히 선형 결정 경계가 적절한 경우에 유용하게 사용됩니다."
      ],
      "metadata": {
        "id": "C9sgpH0P7omH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XG Boost\n",
        "\n",
        "XGBoost(Extreme Gradient Boosting)는 그라디언트 부스팅(Gradient Boosting) 알고리즘의 하나로, 트리 기반의 앙상블 학습 알고리즘입니다. XGBoost는 대회와 실제 응용 프로그램에서 널리 사용되며, 다양한 유형의 데이터셋에서 높은 성능을 보입니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **부스팅 알고리즘:** 부스팅은 약한 학습자(Weak Learner)라 불리는 간단한 모델들을 순차적으로 학습시켜 강력한 모델을 만들어가는 앙상블 기법입니다. XGBoost는 이러한 부스팅 기법을 사용하여 여러 결정 트리를 조합하여 강력한 모델을 형성합니다.\n",
        "\n",
        "2. **트리 알고리즘:** XGBoost는 결정 트리를 기반으로 하는 앙상블 모델입니다. 각 트리는 이전 트리의 오차를 보완하도록 학습됩니다.\n",
        "\n",
        "3. **그라디언트 부스팅:** 그라디언트 부스팅은 손실 함수의 그라디언트(기울기)를 이용하여 모델을 학습하는 방식입니다. XGBoost는 그라디언트 부스팅의 효율적인 구현을 통해 높은 성능을 제공합니다.\n",
        "\n",
        "4. **규제(Regularization):** XGBoost는 규제를 통해 모델의 복잡성을 제어합니다. 이는 모델이 과적합(Overfitting)되는 것을 방지하고 일반적으로 좋은 성능을 내도록 도움을 줍니다.\n",
        "\n",
        "5. **특징 중요도:** XGBoost는 학습된 모델에서 각 특징이 예측에 얼마나 중요한 역할을 하는지를 계산하여 특징 중요도를 제공합니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "XGBoost는 다양한 분야에서 사용됩니다. 주로 다음과 같은 문제에 적용됩니다:\n",
        "\n",
        "- **분류(Classification):** 이진 분류나 다중 클래스 분류 문제에 사용됩니다.\n",
        "  \n",
        "- **회귀(Regression):** 수치형 목표 변수를 예측하는 회귀 문제에도 적용 가능합니다.\n",
        "  \n",
        "- **랭킹(Ranking):** 검색 엔진이나 추천 시스템에서 사용되어 개체의 상대적인 중요도를 학습할 수 있습니다.\n",
        "\n",
        "XGBoost는 성능이 우수하고 많은 하이퍼파라미터 튜닝 옵션을 제공하여 다양한 데이터셋에 적용할 수 있습니다."
      ],
      "metadata": {
        "id": "VxSzS9oNzgNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "\n",
        "랜덤 포레스트(Random Forest)는 앙상블 학습(Ensemble Learning) 기법 중 하나로, 의사 결정 트리(Decision Tree)의 앙상블을 구성하여 강력하고 안정적인 모델을 만드는 알고리즘입니다. 여러 개의 의사 결정 트리를 만들고, 각 트리의 예측을 종합함으로써 과적합을 줄이고 높은 성능을 달성할 수 있습니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **부트스트랩 샘플링 (Bootstrap Sampling):** 랜덤 포레스트는 각 트리를 학습할 때, 데이터의 일부를 무작위로 선택하여 사용합니다. 이는 중복이 허용되는 샘플링으로, 부트스트랩 샘플링이라고 불립니다. 이를 통해 각 트리가 다양한 데이터 부분 집합에서 학습되어 다양성을 증가시킵니다.\n",
        "\n",
        "2. **랜덤 특성 선택 (Random Feature Selection):** 각 의사 결정 트리를 학습할 때, 특성들 중 일부를 무작위로 선택하여 사용합니다. 이는 트리 간의 상관 관계를 줄이고 다양성을 증가시킵니다.\n",
        "\n",
        "3. **앙상블 학습:** 여러 개의 의사 결정 트리를 독립적으로 학습하고, 각 트리의 예측을 평균이나 다수결 등의 방식으로 종합하여 최종 예측을 만듭니다.\n",
        "\n",
        "4. **고차원 데이터와 높은 차원의 특성을 다룰 수 있음:** 다양한 데이터 유형 및 특성에 적용 가능하며, 범주형과 수치형 특성 모두 다룰 수 있습니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "랜덤 포레스트는 다양한 분야에서 사용되며, 특히 다음과 같은 상황에서 효과적입니다:\n",
        "\n",
        "- **분류 및 회귀 문제:** 둘 다 다룰 수 있으며, 분류 문제에서 특히 높은 성능을 보입니다.\n",
        "\n",
        "- **이상치 탐지:** 이상치 탐지에도 적용 가능하며, 개별 트리의 예측을 기반으로 이상치를 식별할 수 있습니다.\n",
        "\n",
        "- **텍스트 및 이미지 데이터:** 다양한 데이터 유형에 적용 가능하며, 특히 텍스트와 이미지 데이터에서 효과적입니다.\n",
        "\n",
        "랜덤 포레스트는 강력하면서도 쉽게 사용할 수 있는 모델로, 많은 상황에서 좋은 기본 모델로 활용됩니다."
      ],
      "metadata": {
        "id": "hM28FVv_73V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Ada Boost\n",
        "\n",
        "AdaBoost(Adaptive Boosting)는 약한 학습자(Weak Learner)들을 조합하여 강력한 학습 모델을 만드는 앙상블 학습 알고리즘 중 하나입니다. AdaBoost는 부분 데이터셋에 가중치를 부여하면서 순차적으로 모델을 학습시키고, 학습이 어려운 샘플에 더 많은 가중치를 주어 다음 모델이 이를 더 잘 학습할 수 있도록 하는 특징을 가지고 있습니다.\n",
        "\n",
        "### 주요 특징:\n",
        "\n",
        "1. **가중치를 부여한 샘플링:** AdaBoost는 각 학습 단계에서 이전 모델이 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 이를 더욱 잘 학습할 수 있도록 합니다.\n",
        "\n",
        "2. **약한 학습자 사용:** AdaBoost는 약한 학습자(Weak Learner)를 사용합니다. 이는 학습 오차가 50% 미만인, 조금 더 나은 성능을 가진 모델로서 일반적으로 의사 결정 트리(Decision Tree)가 사용됩니다.\n",
        "\n",
        "3. **앙상블 학습:** 각 학습 단계에서 새로운 모델을 추가하고, 이전 모델들의 가중 평균을 통해 최종 예측을 만들어냅니다.\n",
        "\n",
        "4. **이진 분류에 적합:** 주로 이진 분류 문제에 사용되며, 다중 클래스 분류 문제에는 확장하여 사용할 수 있습니다.\n",
        "\n",
        "### 동작 원리:\n",
        "\n",
        "1. **첫 번째 모델 학습:** 초기에는 모든 샘플에 동일한 가중치를 부여하고, 첫 번째 모델을 학습시킵니다.\n",
        "\n",
        "2. **오분류된 샘플에 가중치 부여:** 이전 모델이 잘못 분류한 샘플에 대해 가중치를 높여줍니다.\n",
        "\n",
        "3. **새로운 모델 학습:** 새로운 가중치를 적용하여 다음 모델을 학습시킵니다.\n",
        "\n",
        "4. **반복:** 위 과정을 미리 정의한 횟수(또는 정확도가 충분히 높아질 때까지) 반복합니다.\n",
        "\n",
        "5. **앙상블 구성:** 각 모델의 예측을 가중 평균하여 최종 예측을 만듭니다.\n",
        "\n",
        "### 사용 분야:\n",
        "\n",
        "AdaBoost는 다양한 분야에서 사용되며, 특히 다음과 같은 상황에서 효과적입니다:\n",
        "\n",
        "- **이진 분류 문제:** 주로 이진 분류 문제에서 사용되며, 약한 학습자를 사용하므로 간단하고 빠른 모델을 구성할 수 있습니다.\n",
        "\n",
        "- **얼굴 검출, 객체 감지:** 이미지 처리에서 얼굴 검출이나 객체 감지와 같은 작업에 사용될 수 있습니다.\n",
        "\n",
        "- **자연어 처리(NLP):** 텍스트 분류 문제에서도 효과적으로 사용될 수 있습니다.\n",
        "\n",
        "AdaBoost는 간단하면서도 성능이 좋은 앙상블 학습 알고리즘 중 하나이지만, 이상치(outliers)에 민감할 수 있으며, 노이즈 데이터에 과적합할 가능성이 있습니다."
      ],
      "metadata": {
        "id": "3lsITqFN7626"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s3Foh4STzgWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test =  train_test_split(X, y, random_state=0, test_size=0.2)\n",
        "\n",
        "log_clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter = 5000, multi_class='multinomial').fit(X_train, y_train)\n",
        "log_result = log_clf.predict(X_train)\n",
        "#print(log_clf.predict_proba(X_test))\n",
        "log_prob = log_clf.predict_proba(X_test)\n",
        "print(log_clf.score(X_train, y_train))\n",
        "print(log_clf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EjSTFySZ7-s",
        "outputId": "8366151f-f3d1-4cae-b430-52f130b77fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5272127542605828\n",
            "0.5340659340659341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_clf = XGBClassifier(max_depth=3, n_estimators=250)\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "#print(X.columns.tolist())\n",
        "#print(xgb_clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(xgb_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(xgb_clf.score(X_test,y_test)*100,2)))\n",
        "\n",
        "xgb_prob = xgb_clf.predict_proba(X_test)\n",
        "\n",
        "\n",
        "print(xgb_clf.score(X_test, y_test))\n",
        "print(roc_auc_score(y_test,xgb_clf.predict(X_test)))\n",
        "#xgb_clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uR3xaYaEAg",
        "outputId": "76adbe91-b5fe-47ea-b176-6fefff96d12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "accuracy with training 99.18%\n",
            "accuracy with testing 80.44%\n",
            "0.8043956043956044\n",
            "0.8035214824022998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_clf = RandomForestClassifier(n_estimators=250, random_state=0, max_depth=3)\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "RF_clf.fit(X_train, y_train)\n",
        "#print(X_train.columns.tolist())\n",
        "#print(RF_clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(RF_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(RF_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,RF_clf.predict(X_test)))\n",
        "\n",
        "y_pred = RF_clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "218O2ISxaVpU",
        "outputId": "c586c4bc-844b-41c8-886b-1aceb0a280b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "==================================================\n",
            "accuracy with training 74.0%\n",
            "accuracy with testing 72.09%\n",
            "0.7166789682231374\n",
            "[[139  72]\n",
            " [ 55 189]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.66      0.69       211\n",
            "           1       0.72      0.77      0.75       244\n",
            "\n",
            "    accuracy                           0.72       455\n",
            "   macro avg       0.72      0.72      0.72       455\n",
            "weighted avg       0.72      0.72      0.72       455\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(n_estimators=200, learning_rate=0.1,random_state=0)\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "ada_clf.fit(X_train, y_train)\n",
        "#print(clf.feature_importances_)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"accuracy with training {}%\".format(round(ada_clf.score(X_train,y_train)*100,2)))\n",
        "print(\"accuracy with testing {}%\".format(round(ada_clf.score(X_test,y_test)*100,2)))\n",
        "print(roc_auc_score(y_test,ada_clf.predict(X_test)))\n",
        "ADA_prob = ada_clf.predict_proba(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVIsyp6TaYKg",
        "outputId": "72b75a8e-219b-4c13-f6ab-3cb8a65576f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "==================================================\n",
            "accuracy with training 73.83%\n",
            "accuracy with testing 72.31%\n",
            "0.7225740035739259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_info = datetime.today().strftime(format=\"%Y-%m-%d\")\n",
        "\n",
        "## 학습한 W2V 모델 저장\n",
        "model.save('/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/word_model/word2vec_{}_{}_{}.model'.format(vec_size, window_num, date_info))\n",
        "\n",
        "## 학습한 머신러닝\n",
        "from joblib import dump, load\n",
        "\n",
        "fname2 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/word_model/xgb_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(xgb_clf, fname2)\n",
        "\n",
        "fname3 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/word_model/RF_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(RF_clf, fname3)\n",
        "\n",
        "fname4 = '/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/word_model/ada_model_{}_{}_{}.pkl'.format(vec_size, window_num, date_info)\n",
        "dump(ada_clf, fname4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvYxRU1Xacch",
        "outputId": "9379ad3f-bcb6-47ba-9cf9-323013b69bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Smart_Data_Analysis_Class_202302_비공개/Week_12/word_model/ada_model_300_3_2023-11-22.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKpxEMzNspUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}